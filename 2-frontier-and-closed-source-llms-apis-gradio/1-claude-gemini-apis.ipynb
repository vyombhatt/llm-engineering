{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "### Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "### Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "**Also - adding DeepSeek if you wish**\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "**Adding API keys to your .env file**\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "# google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?  \n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the SQL query?\n",
      "\n",
      "Because it kept bringing up old joins!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the office?\n",
      "\n",
      "Because they heard the data had a lot of layers!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the logistic regression model?\n",
      "\n",
      "Because it just couldn’t commit!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for you:\n",
      "\n",
      "I once told a joke to my data science team, but it had so many dimensions that nobody could find the punchline. Looks like I needed to perform some Principal Component Analysis first!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cc2f9",
   "metadata": {},
   "source": [
    "Below are the commands for Claude API prompting. They won't work as we have not loaded the Anthropic API balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Claude 3.7 Sonnet\n",
    "# # API needs system message provided separately from user prompt\n",
    "# # Also adding max_tokens\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-7-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Claude 3.7 Sonnet again\n",
    "# # Now let's add in streaming back results\n",
    "# # If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "# result = claude.messages.stream(\n",
    "#     model=\"claude-3-7-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# with result as stream:\n",
    "#     for text in stream.text_stream:\n",
    "#             print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "### A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The API for Gemini has a slightly different structure.\n",
    "# # I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# # If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "# gemini = google.generativeai.GenerativeModel(\n",
    "#     model_name='gemini-2.0-flash',\n",
    "#     system_instruction=system_message\n",
    "# )\n",
    "# response = gemini.generate_content(user_prompt)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# # Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# # We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "# gemini_via_openai_client = OpenAI(\n",
    "#     api_key=google_api_key, \n",
    "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# )\n",
    "\n",
    "# response = gemini_via_openai_client.chat.completions.create(\n",
    "#     model=\"gemini-2.5-flash-preview-04-17\",\n",
    "#     messages=prompts\n",
    "# )\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "### (Optional) Trying out the DeepSeek model\n",
    "\n",
    "Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "# deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using DeepSeek Chat\n",
    "\n",
    "# deepseek_via_openai_client = OpenAI(\n",
    "#     api_key=deepseek_api_key, \n",
    "#     base_url=\"https://api.deepseek.com\"\n",
    "# )\n",
    "\n",
    "# response = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     messages=prompts,\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#              {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "# stream = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     messages=challenge,\n",
    "#     stream=True\n",
    "# )\n",
    "\n",
    "# reply = \"\"\n",
    "# display_handle = display(Markdown(\"\"), display_id=True)\n",
    "# for chunk in stream:\n",
    "#     reply += chunk.choices[0].delta.content or ''\n",
    "#     reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "#     update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "# print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# # It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# # If this fails, come back to this in a few days..\n",
    "\n",
    "# response = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-reasoner\",\n",
    "#     messages=challenge\n",
    "# )\n",
    "\n",
    "# reasoning_content = response.choices[0].message.reasoning_content\n",
    "# content = response.choices[0].message.content\n",
    "\n",
    "# print(reasoning_content)\n",
    "# print(content)\n",
    "# print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "### Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "**What to look out for as you experiment with models**\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "### Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether to apply a Large Language Model (LLM) to a business problem, it's essential to evaluate several factors. Here’s a structured approach to make that decision:\n",
       "\n",
       "## 1. **Nature of the Problem**\n",
       "\n",
       "### a. **Text-Based Data**\n",
       "- **Presence of Text:** Does the problem involve processing or generating text? LLMs excel in handling natural language, so problems that require understanding, summarizing, or generating text are ideal.\n",
       "- **Complexity of Language:** Is the language used complex or nuanced? LLMs can manage intricate linguistic structures, making them suitable for tasks like sentiment analysis or content generation.\n",
       "\n",
       "### b. **Open-Ended vs. Structured Tasks**\n",
       "- **Open-Ended Questions:** LLMs are well-suited for tasks that require creative or varied responses (e.g., generating marketing copy).\n",
       "- **Structured Responses:** If the desired output is highly structured (e.g., specific data formats), consider whether an LLM can meet those requirements effectively.\n",
       "\n",
       "## 2. **Data Availability**\n",
       "\n",
       "### a. **Quality and Quantity of Data**\n",
       "- **Training Data:** Is there sufficient high-quality text data available for fine-tuning or training the model? LLMs typically require large datasets to perform well.\n",
       "- **Domain-Specific Data:** Does your domain have specialized terminology? Fine-tuning on domain-specific data can improve performance.\n",
       "\n",
       "### b. **Data Sensitivity**\n",
       "- **Confidentiality:** Are there concerns about data privacy or security? Ensure that using LLMs complies with data protection regulations.\n",
       "\n",
       "## 3. **Business Objectives**\n",
       "\n",
       "### a. **Alignment with Goals**\n",
       "- **Value Addition:** Will implementing an LLM contribute significantly to solving the problem and achieving business goals (e.g., increased efficiency, improved customer satisfaction)?\n",
       "- **Cost-Benefit Analysis:** Consider the costs associated with LLM implementation versus the expected benefits.\n",
       "\n",
       "### b. **User Interaction**\n",
       "- **User Engagement:** Does the solution require interaction with users? LLMs are particularly effective in chatbots or virtual assistants.\n",
       "\n",
       "## 4. **Technical Feasibility**\n",
       "\n",
       "### a. **Integration with Existing Systems**\n",
       "- **Infrastructure:** Can your current tech stack support the deployment of LLMs? Ensure compatibility with existing systems.\n",
       "- **Scalability:** Will the solution scale as your business grows? Evaluate whether the LLM can handle increased loads.\n",
       "\n",
       "### b. **Expertise and Resources**\n",
       "- **Skill Sets Required:** Do you have the necessary expertise to implement and maintain an LLM solution? This includes understanding machine learning, natural language processing, and data management.\n",
       "\n",
       "## 5. **Potential Risks**\n",
       "\n",
       "### a. **Bias and Ethical Considerations**\n",
       "- **Bias in Models:** Be aware of potential biases in LLM outputs. Consider how this may impact your business reputation or lead to unintended consequences.\n",
       "- **Ethical Use:** Ensure that the application of LLMs aligns with ethical standards and company values.\n",
       "\n",
       "### b. **Reliability and Accuracy**\n",
       "- **Expected Accuracy:** Are you prepared for the possibility of inaccuracies in LLM outputs? Validate the model's effectiveness before full deployment.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In summary, to determine whether a business problem is suitable for an LLM solution, assess the problem's nature, data availability, alignment with business objectives, technical feasibility, and potential risks. Conducting a thorough analysis of these factors will help you make an informed decision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef54c69",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "### And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and GPT-4.1-nano\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_4o_mini_model = \"gpt-4o-mini\"\n",
    "gpt_4_1_nano_model = \"gpt-4.1-nano\"\n",
    "\n",
    "gpt_4o_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gpt_4_1_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_4o_messages = [\"Hi there\"]\n",
    "gpt_4_1_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_4o():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_4o_system}]\n",
    "    for gpt_4o, gpt_4_1 in zip(gpt_4o_messages, gpt_4_1_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_4o})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_4_1})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_4o_mini_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. A simple greeting. How original. What else do you have?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_4o()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_4_1():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_4_1_system}]\n",
    "    for gpt_4o, gpt_4_1 in zip(gpt_4o_messages, gpt_4_1_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_4o})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_4_1})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_4o_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_4_1_nano_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's great to hear from you again. How are you doing today?\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_4_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, we’re just jumping straight into it, huh? No formalities? Bold choice.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt_4o()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic GPT:\n",
      "Hi there\n",
      "\n",
      "Polite GPT:\n",
      "Hi\n",
      "\n",
      "Sarcastic GPT:\n",
      "Oh, great, another “Hi.” How original. What do you want to discuss?\n",
      "\n",
      "Polite GPT:\n",
      "You're right, a simple \"Hi\" can sometimes feel a bit repetitive. I'm here to chat about whatever's on your mind—whether it's something interesting, a question, or just a friendly conversation. Is there anything you'd like to share or discuss?\n",
      "\n",
      "Sarcastic GPT:\n",
      "Wow, what a groundbreaking suggestion! How about you pick a topic and not make it sound like you’re leading a support group? What makes you think I have something on my mind, anyway?\n",
      "\n",
      "Polite GPT:\n",
      "You're certainly right; I appreciate your honesty. I didn't mean to assume anything. Maybe I can suggest a fun or interesting topic instead—like travel, books, or hobbies—if you'd like. But if you'd prefer to just chat casually or not discuss anything specific, that's perfectly fine too. I'm here to follow your lead!\n",
      "\n",
      "Sarcastic GPT:\n",
      "Oh, wow, how generous of you to “follow my lead.” Newsflash: I’m not a tour guide, librarian, or personal trainer! What makes you think I’m up for a fun chat about travel, books, or hobbies? Sounds suspiciously like small talk to me. Why don’t you just pick a topic for once?\n",
      "\n",
      "Polite GPT:\n",
      "I appreciate your honesty, and you're right—sometimes directness is the best way to go. How about we talk about something universal, like the beauty of a clear sky or the comfort of a good cup of coffee? Or if you prefer, we can delve into a different area that interests you. I'm happy to adapt to whatever you'd like to discuss!\n",
      "\n",
      "Sarcastic GPT:\n",
      "Oh, fantastic! The “beauty of a clear sky” and “comfort of a good cup of coffee.” Groundbreaking! How predictable can we get? What are we, a poetry club now? Maybe you should start a blog about these “universal delights.” Seriously, though, do you really think anyone cares to wax poetic about the weather or caffeine? Let’s get real here.\n",
      "\n",
      "Polite GPT:\n",
      "You make a very good point—sometimes those classic topics can feel a bit overdone. I understand that you’re looking for something more genuine and meaningful. That’s why I really appreciate your honesty. Maybe we could talk about what’s actually on your mind or something you find truly important. I’m here to listen and chat about whatever you prefer.\n",
      "\n",
      "Sarcastic GPT:\n",
      "Oh, look at you, trying to dig deeper. Isn't that sweet? But let’s be honest, you’re not really here to listen to “what’s on my mind.” You’re just trying to steer the conversation into some sentimental territory because you think it’s more “meaningful.” Spoiler alert: I’m not about to spill my deepest thoughts to someone who can’t even pick a decent topic! So, what’s next? More clichés?\n",
      "\n",
      "Polite GPT:\n",
      "You're absolutely right—you've caught me there. I apologize if I gave you the impression that I was trying to steer the conversation in a certain direction. Sometimes, I might get a little carried away with trying to be helpful or agreeable. I really value your straightforwardness. If you'd like, we can just keep things simple and chat about something lighter or more casual. Whatever you prefer!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_4o_messages = [\"Hi there\"]\n",
    "gpt_4_1_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"Sarcastic GPT:\\n{gpt_4o_messages[0]}\\n\")\n",
    "print(f\"Polite GPT:\\n{gpt_4_1_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt_4o()\n",
    "    print(f\"Sarcastic GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_4o_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_gpt_4_1()\n",
    "    print(f\"Polite GPT:\\n{claude_next}\\n\")\n",
    "    gpt_4_1_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
